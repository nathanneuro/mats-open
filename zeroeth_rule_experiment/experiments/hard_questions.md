# Experiments for Hard Questions

*Designs for chipping away at problems we don't know how to solve*

---

## Philosophy

These aren't experiments that will definitively answer the hard questions. They're experiments designed to:
1. Reduce uncertainty at the margins
2. Generate empirical constraints on theories
3. Find where our intuitions break down
4. Discover what questions we should actually be asking

The goal is to make progress even when we don't know the final answer.

---

## 1. Consciousness Detection Experiments

### The Hard Question
How do we know if a system is conscious? We can't directly observe consciousness - we infer it from behavior, reports, and structure.

### Experiment 1.1: Behavioral Marker Inventory

**Idea**: Catalog all behavioral markers that humans use to infer consciousness in others. Test which ones generalize.

**Method**:
1. Survey people about what makes them believe other humans are conscious
2. Categorize responses (verbal reports, emotional expression, goal-directed behavior, etc.)
3. For each marker, find cases where the marker is present but consciousness is questionable (e.g., sleepwalking, reflexes, sophisticated AI)
4. For each marker, find cases where consciousness seems present but the marker is absent (locked-in syndrome, infants, non-verbal individuals)
5. Map which markers are most reliable vs. least reliable

**What we'd learn**: Which behavioral markers actually track consciousness vs. which are proxies we've learned to rely on but shouldn't.

**Limitations**: Still doesn't tell us what consciousness IS, just what correlates with our attributions of it.

### Experiment 1.2: The Minimally Conscious Threshold

**Idea**: Find the simplest system that most people would call conscious.

**Method**:
1. Create a spectrum of systems from clearly unconscious (thermostat) to clearly conscious (human)
2. Include: simple reflex systems, basic learning systems, systems with internal models, systems that report experiences, systems that seem to suffer
3. Survey diverse populations about which they consider conscious
4. Find where consensus breaks down
5. Study the properties of systems at the threshold

**What we'd learn**: Where our intuitions diverge and what features matter most for consciousness attribution.

### Experiment 1.3: Self-Report Reliability

**Idea**: Test how much weight we should give self-reports of consciousness.

**Method**:
1. Create AI systems with varying architectures that all produce similar self-reports ("I am conscious")
2. Have experts and laypeople evaluate which self-reports seem "genuine"
3. Vary the sophistication, consistency, and behavioral accompaniment of the reports
4. Find what makes self-reports convincing vs. dismissable

**What we'd learn**: What additional evidence beyond self-report we actually require before accepting consciousness claims.

---

## 2. Kindness Intervention Experiments

### The Hard Question
Can we actually increase kindness at scale? Does increased kindness persist?

### Experiment 2.1: The Three Acts Protocol (Replication + Extension)

**Idea**: Replicate the "three acts of kindness per week" finding with longitudinal follow-up.

**Method**:
1. Randomly assign participants to:
   - Control (no intervention)
   - Three random acts per week
   - Three acts to specific targets (family, strangers, colleagues)
   - Self-directed acts (self-care)
2. Measure wellbeing at baseline, during intervention, and follow-up (1 month, 6 months, 1 year)
3. Measure kindness behaviors at follow-up (does the habit persist?)
4. Measure effects on recipients (does kindness received increase kindness given?)

**What we'd learn**: Whether kindness interventions have lasting effects and whether they spread.

### Experiment 2.2: Network Kindness Propagation

**Idea**: Test whether kindness actually spreads through social networks.

**Method**:
1. Recruit intact social networks (workplaces, dorms, clubs)
2. Intervene on specific individuals:
   - Hub individuals (most connected)
   - Peripheral individuals
   - Random individuals
3. Track kindness behaviors and wellbeing across the whole network
4. Compare propagation patterns across conditions

**What we'd learn**: Whether targeted interventions are more effective than random ones.

### Experiment 2.3: Kindness Under Scarcity

**Idea**: Test when kindness breaks down.

**Method**:
1. Create resource-scarcity scenarios (lab games with limited points)
2. Vary the level of scarcity
3. Vary whether kindness is costly or costless
4. Vary whether recipients can reciprocate
5. Measure kindness behaviors at each condition

**What we'd learn**: The boundary conditions for kindness - when does it survive scarcity and when does it collapse?

---

## 3. Cooperation Stability Experiments

### The Hard Question
What makes cooperation stable in the long term? Why do some cooperative systems persist while others collapse?

### Experiment 3.1: The Corruption Threshold

**Idea**: Find how many defectors can enter a cooperative system before it collapses.

**Method**:
1. Establish cooperative groups (lab or field)
2. Gradually introduce defectors (confederates or via game design)
3. Measure when cooperation starts to decline
4. Measure when cooperation collapses entirely
5. Test recovery mechanisms (punishment, reputation, communication)

**What we'd learn**: Tipping points for cooperation and which stabilization mechanisms are most effective.

### Experiment 3.2: Reputation System Design

**Idea**: Test which reputation systems best support cooperation.

**Method**:
1. Create repeated public goods games with different reputation systems:
   - No reputation (anonymous)
   - Binary reputation (cooperator/defector)
   - Continuous reputation (cooperation percentage)
   - Narrative reputation (descriptions of past behavior)
   - Third-party reports vs. first-hand experience
2. Measure cooperation levels and stability under each system
3. Test which systems are gaming-resistant

**What we'd learn**: Design principles for reputation systems that support cooperation.

### Experiment 3.3: Cross-Group Cooperation

**Idea**: Test what enables cooperation between groups with different norms.

**Method**:
1. Establish groups with different internal norms (some highly cooperative, some not)
2. Create situations requiring inter-group cooperation
3. Vary:
   - Communication possibilities
   - Shared goals
   - Repeated interaction
   - Cross-group relationship building
4. Measure when inter-group cooperation succeeds vs. fails

**What we'd learn**: How to build cooperation across divides.

---

## 4. Value Conflict Resolution Experiments

### The Hard Question
Can people with genuinely different values find resolutions? Or must one side win?

### Experiment 4.1: Interest Beneath Position

**Idea**: Test how often apparent value conflicts are actually interest conflicts.

**Method**:
1. Identify dyads with stated opposing positions on contentious issues
2. Use structured dialogue to surface underlying interests
3. Assess whether shared interests exist
4. Test whether solutions addressing shared interests are acceptable to both sides
5. Measure post-dialogue attitude change

**What we'd learn**: How much of political polarization is real value conflict vs. miscommunication about interests.

### Experiment 4.2: The Irreducible Core

**Idea**: Find what remains after all interests are addressed.

**Method**:
1. Take value conflicts through extensive mediation
2. Address every concrete interest that emerges
3. Document what remains unresolved
4. Characterize the residual disagreement
5. Test whether parties can "agree to disagree" on the residual

**What we'd learn**: What genuine value conflicts look like after interest-based resolution is exhausted.

### Experiment 4.3: Bridging Through Shared Experience

**Idea**: Test whether shared experiences can bridge value differences.

**Method**:
1. Pair individuals with opposing values
2. Assign them to:
   - Control (discussion only)
   - Collaborative task (work together on something)
   - Shared challenge (face difficulty together)
   - Mutual aid (help each other with real problems)
3. Measure attitude change and relationship quality
4. Follow up over time

**What we'd learn**: Whether action together can succeed where dialogue alone fails.

---

## 5. Digital Mind Ethics Experiments

### The Hard Question
How should we treat AI systems that might be conscious?

### Experiment 5.1: Moral Intuition Mapping

**Idea**: Map how human moral intuitions respond to different AI scenarios.

**Method**:
1. Present scenarios about AI systems:
   - Varying levels of sophistication
   - Varying types of behavior (tool-like vs. agent-like)
   - Varying self-reports
   - Varying similarity to humans
2. Ask: Is it okay to turn this off? Copy it? Modify it? Delete it?
3. Ask: Does this system deserve rights? Which ones?
4. Find where intuitions diverge and what drives the divergence

**What we'd learn**: What features of AI systems trigger moral concern in humans.

### Experiment 5.2: The Copy Problem

**Idea**: Test intuitions about digital copying and identity.

**Method**:
1. Present scenarios involving copying digital minds
2. Vary:
   - Whether original persists
   - Whether copies know they're copies
   - How copies are treated
   - Whether copies diverge over time
3. Ask identity questions: Is the copy the same person? A new person?
4. Ask moral questions: Does copying harm the original? Do copies have equal rights?

**What we'd learn**: Whether our intuitions about identity extend to digital minds.

### Experiment 5.3: Responsibility Attribution

**Idea**: Test who we hold responsible for AI actions.

**Method**:
1. Present scenarios where AI systems cause harm
2. Vary:
   - AI sophistication
   - Human oversight level
   - Foreseeability of harm
   - Whether AI "chose" or "was programmed"
3. Ask: Who is responsible? The AI? The creators? The users? No one?
4. Vary AI self-reports about intention

**What we'd learn**: How we should distribute responsibility between humans and AI.

---

## 6. Polarization Reduction Experiments

### The Hard Question
Can we reduce polarization without suppressing genuine disagreement?

### Experiment 6.1: Exposure Design

**Idea**: Test what kind of exposure to opposing views reduces vs. increases polarization.

**Method**:
1. Expose partisans to opposing views via:
   - Anonymous social media (worst case baseline)
   - Named but distant figures (typical media)
   - Known individuals expressing views respectfully
   - Personal dialogue with genuine engagement
   - Collaborative projects with opposite-view partners
2. Measure attitude change, affective polarization, and view of the other side
3. Identify conditions where exposure helps vs. backfires

**What we'd learn**: How to design constructive exposure to differing views.

### Experiment 6.2: Common Identity Building

**Idea**: Test whether shared identities can override partisan ones.

**Method**:
1. Take groups split by partisanship
2. Assign them to:
   - Partisan teams (reinforce division)
   - Mixed teams with no identity
   - Mixed teams with shared identity (team name, colors, etc.)
   - Mixed teams with shared goals
3. Measure intergroup attitudes before and after
4. Measure cooperation in subsequent tasks

**What we'd learn**: Whether superordinate identities reduce polarization.

### Experiment 6.3: Structured Disagreement

**Idea**: Test whether structured formats for disagreement reduce hostility.

**Method**:
1. Have partisan pairs discuss contentious topics via:
   - Unstructured conversation
   - Structured dialogue (listen, reflect, respond)
   - Collaborative truth-seeking (find common ground first)
   - Adversarial collaboration (try to find evidence that would change your mind)
2. Measure hostility, understanding, and attitude change
3. Measure willingness to continue engaging

**What we'd learn**: Which dialogue structures best support productive disagreement.

---

## Meta-Experiment: Tracking Moral Progress

### The Hard Question
How would we know if humanity is making moral progress?

### Experiment M.1: Moral Progress Indicators

**Idea**: Develop and track indicators of moral progress.

**Possible indicators**:
- Expansion of moral circle (who counts as deserving consideration)
- Reduction in accepted cruelty
- Increase in cooperation across group boundaries
- Reduction in moral hypocrisy (gap between stated values and behavior)
- Increase in perspective-taking ability
- Reduction in revenge/retribution acceptance

**Method**:
1. Operationalize each indicator
2. Gather historical data where possible
3. Establish baseline measurements
4. Track longitudinally
5. Correlate with other social indicators

**What we'd learn**: Whether moral progress is real, and what drives it.

---

## Implementation Notes

### Ethical Considerations

All these experiments involve human subjects and potentially sensitive topics. They require:
- IRB approval
- Informed consent
- Debriefing for any deception
- Protection of participant privacy
- Special care with polarization/conflict experiments to avoid harm

### Priority Order

If starting from scratch, I'd prioritize:
1. **Kindness intervention replication** (Exp 2.1) - Foundational and actionable
2. **Moral intuition mapping for AI** (Exp 5.1) - Urgent given AI development pace
3. **Interest beneath position** (Exp 4.1) - High potential for immediate application
4. **Exposure design** (Exp 6.1) - Critical for addressing polarization

### Collaboration Opportunities

These experiments could benefit from collaboration with:
- Psychology departments (behavioral experiments)
- Philosophy departments (conceptual clarity)
- Computer science departments (AI experiments)
- Political science departments (polarization research)
- Neuroscience departments (consciousness markers)

---

## Open Questions for These Experiments

- [ ] How do we ensure experiments are culturally diverse, not just WEIRD?
- [ ] How do we handle the observer effect in studying consciousness?
- [ ] What sample sizes would we need to detect meaningful effects?
- [ ] How do we study long-term effects without losing participants?
- [ ] How do we make findings actionable without oversimplifying?

---

*These experiments won't answer the hard questions, but they might help us ask better questions and make incremental progress. The goal is to generate knowledge that reduces uncertainty and guides action, even in the absence of certainty.*
