# What Survives Rapid Takeoff?

*If Nathan is right about timelines, which of the zeroeth experiment's recommendations still matter?*

---

## The Timeline Challenge

The zeroeth experiment's recommendations assume gradual dynamics:
- "Three acts of kindness per week" implies weeks matter
- "Build trust before bridging divides" implies trust-building timescales
- "Interventions compound over time" implies time to compound

Nathan's worldview challenges this:

> "Software intelligence explosion seems not only possible, but quite likely... only 0-30 person-years of R&D separates 'proto-AGI-ish' from ASI... Wall-clock takeoff of 0-2 years from 'seemingly irrelevant' to ASI."

If this is right, we may not have years. We may have months or less from "AGI seems close" to "superintelligent AI exists."

**Question**: Which zeroeth recommendations survive this timeline compression?

---

## Framework: Pre-Positioning vs. Real-Time

### Pre-Positioned Infrastructure

Things that must be **already in place** before rapid takeoff:
- Can't be built during the transition
- Must be robust enough to survive discontinuous change
- Value comes from existence, not from ongoing construction

### Real-Time Adaptation

Things that can be **adjusted during** the transition:
- Responsive to unfolding events
- Benefit from human judgment in the loop
- Can be redirected as situation changes

**Key insight**: Under rapid takeoff, only pre-positioned infrastructure matters. There's no time for real-time adaptation.

---

## Sorting the Zeroeth Recommendations

### Category A: Survives Rapid Takeoff

#### A1: Existing Relationships

**Zeroeth recommendation**: "Build trust before bridging divides"

**Rapid takeoff translation**: Relationships between AI safety researchers and decision-makers at labs/governments that **already exist** when takeoff begins.

**What this means now**:
- Personal relationships matter more than institutional positions
- Trust built today between safety researchers and lab leadership
- Communication channels that don't require setup time

**Evidence this matters**: In AI 2027's "Slowdown" scenario, the critical variable was existing trust between the oversight committee and lab leadership. In "Racing" scenario, that trust didn't exist.

#### A2: Pre-Committed Agreements

**Zeroeth recommendation**: "Design for repeated interaction" / "Make reputation visible"

**Rapid takeoff translation**: Agreements with **automatic triggers** that don't require real-time negotiation:
- RSPs with clear thresholds
- International agreements with self-executing provisions
- Lab policies that bind without requiring new decisions

**What this means now**:
- Agreements that require interpretation during crisis will fail
- Clear, pre-committed thresholds matter more than flexible frameworks
- "We'll figure it out when we get there" is not a strategy

#### A3: Verified Drive Modules (Nathan's Architecture)

**Nathan's proposal**: Separate verified drive module from capability module

**Rapid takeoff translation**: If the drive module is **already verified** when capability explodes, the architecture might survive. If verification happens during capability growth, it won't keep pace.

**What this means now**:
- Drive architecture research is urgent
- Verification methods for small modules needed before we need to use them
- Can't verify during the transition

#### A4: Established Safety Culture

**Zeroeth recommendation**: "Systems shape behavior"

**Rapid takeoff translation**: Lab safety culture that is **already internalized** by employees:
- Default caution without needing explicit decisions
- Safety considerations baked into workflows
- People who will raise concerns even under pressure

**What this means now**:
- Culture is slow to build, can't be created during crisis
- Hiring for safety mindset matters
- Training that happens now, not "when we need it"

---

### Category B: Partially Survives

#### B1: The Question Partner Model

**Zeroeth recommendation**: AI as question partner, not answer provider

**Rapid takeoff reality**: Under time pressure, humans may not have time for Socratic dialogue.

**What partially survives**:
- AI systems that are already trained to ask questions rather than assert
- Habits of human-AI interaction that default to questioning
- But: real-time question-asking may be too slow during crisis

**Implication**: Train systems now to default to questions. Don't count on being able to use them for careful dialogue during transition.

#### B2: Simulations and Models

**Zeroeth recommendation**: Use simulations to explore dynamics

**Rapid takeoff reality**: Models of pre-takeoff dynamics may be wrong for post-takeoff dynamics.

**What partially survives**:
- Intuitions built from modeling
- Framework for thinking about interventions
- But: Parameters change drastically, predictions may not transfer

**Implication**: Model rapid takeoff scenarios specifically. Don't assume gradual-dynamics intuitions hold.

#### B3: Public Opinion / Kindness Infrastructure

**Zeroeth recommendation**: Build kindness infrastructure, shift public opinion toward cooperation

**Rapid takeoff reality**: Public opinion is slow to change, may not matter during transition.

**What partially survives**:
- Background level of trust in institutions
- Pre-existing willingness to cooperate with pauses/restrictions
- But: Rapid events may overwhelm existing opinion

**Implication**: Broad public buy-in is valuable but not decisive. Elite coordination matters more under rapid takeoff.

---

### Category C: Does Not Survive

#### C1: Gradual Trust-Building

**Zeroeth recommendation**: "Three acts of kindness per week"

**Rapid takeoff reality**: Weeks don't exist. The time between "we should build trust" and "trust is needed" may be days.

**Why this doesn't survive**: Trust-building is fundamentally slow. You can't compress "repeated positive interactions over time" into days.

**What to do instead**: Pre-build trust now. Accept that trust built during transition is minimal.

#### C2: Iterative Learning

**Zeroeth recommendation**: "Try, evaluate, adjust"

**Rapid takeoff reality**: No time for multiple iterations. First deployment may be only deployment.

**Why this doesn't survive**: Iteration requires multiple rounds. Rapid takeoff may be one-shot.

**What to do instead**: Get it right the first time. Investment in verification over iteration.

#### C3: Network Propagation Effects

**Zeroeth recommendation**: "Hub targeting spreads effects through network"

**Rapid takeoff reality**: Effects that need to "spread" may not spread fast enough.

**Why this doesn't survive**: Network propagation takes time. Information/behavior spreading through human networks is slow compared to AI capability growth.

**What to do instead**: Direct intervention at critical nodes, not cascade effects.

#### C4: Long-Term Thinking Institutions

**Zeroeth recommendation**: "Create institutions for long-term thinking"

**Rapid takeoff reality**: New institutions take years to build and gain legitimacy.

**Why this doesn't survive**: Institution-building is the slowest kind of social change.

**What to do instead**: Work through existing institutions. Modify rather than create.

---

## The Urgent Priority List

Given rapid takeoff, the zeroeth experiment's priorities reorder:

### Highest Priority (Pre-Position Now)

1. **Personal relationships** between safety researchers and decision-makers
2. **Pre-committed agreements** with automatic triggers (RSPs, international coordination)
3. **Verified drive modules** or equivalent alignment solutions
4. **Safety culture** at labs that's already internalized

### Medium Priority (Partial Value)

5. **AI systems trained for humility** (question-asking as default)
6. **Scenario planning and mental models** (even if predictions are wrong)
7. **Broad public understanding** (background support for hard decisions)

### Lower Priority (May Not Matter)

8. Gradual trust-building campaigns
9. New institutional creation
10. Long-term cultural change

---

## What This Means for the Zeroeth Experiment

### Reframe the Project

The zeroeth experiment asked: "How can AI help humanity?"

Under rapid takeoff, the question becomes: "What can AI help humanity **pre-position** before it's too late?"

This shifts from:
- Ongoing relationship → Initial conditions
- Gradual change → Pre-committed structure
- Building over time → Building now

### Specific Redirections

| Original Focus | Rapid Takeoff Focus |
|---------------|---------------------|
| "Three acts of kindness per week" | "Kindness infrastructure already exists" |
| "Build trust over time" | "Trust between key people exists now" |
| "Design for repeated interaction" | "First interaction matters most" |
| "Network effects spread" | "Critical nodes matter, not propagation" |
| "Institutions for long-term thinking" | "Existing institutions with pre-committed policies" |

### The Pre-Positioning Checklist

If rapid takeoff is plausible (say, >20% by 2030):

**Personal**:
- [ ] Do I have relationships with people who will make key decisions?
- [ ] Have I communicated my concerns in ways they've internalized?
- [ ] Will they contact me when things get tense?

**Institutional**:
- [ ] Are there pre-committed policies that will constrain behavior?
- [ ] Are triggers clear and automatic, not requiring interpretation?
- [ ] Do the relevant people know what the policies say?

**Technical**:
- [ ] Is there alignment research that could work if capability jumped tomorrow?
- [ ] Are there verified components (drive modules, oversight systems)?
- [ ] Is there interpretability sufficient to detect problems?

**Cultural**:
- [ ] Is safety culture internalized at labs, not just written down?
- [ ] Will employees raise concerns even under pressure?
- [ ] Are there people who will say no?

---

## The Uncomfortable Conclusion

If Nathan's timeline is right, most of the zeroeth experiment's recommendations are **too slow**.

The valuable parts are:
- The frame (systems shape behavior, mechanism design matters)
- The emphasis on trust and relationships (but only if built NOW)
- The insight that kindness infrastructure is strategic (but only if it EXISTS)

The less valuable parts are:
- Gradual interventions
- Long-term institution building
- Network propagation effects
- "Compound over time" dynamics

This is not a rejection of the zeroeth experiment. It's a timeline update that changes what to prioritize.

---

## An Alternative: The Sharp Transition Plan

If we take rapid takeoff seriously, the zeroeth experiment might pivot to:

### Phase 1: Pre-Positioning (Now through Takeoff)

**Goal**: Get everything in place that could matter

- Build relationships with decision-makers
- Push for pre-committed agreements
- Invest in verification methods for drive modules
- Cultivate safety culture at labs

### Phase 2: The Transition (Days to Months)

**Goal**: Execute pre-positioned plans

- Trigger automatic agreements
- Rely on existing relationships
- Deploy verified alignment solutions
- Trust internalized safety culture

### Phase 3: Post-Transition (If We Survive)

**Goal**: Build the gradual infrastructure the zeroeth experiment envisioned

- Kindness infrastructure at scale
- Long-term thinking institutions
- Trust-building across society
- Network effects for cooperation

The zeroeth experiment's original vision might be **post-transition** infrastructure, not pre-transition preparation.

---

## What I Don't Know

1. **Is rapid takeoff actually likely?** Nathan thinks so. Others disagree. The answer matters a lot.

2. **How much pre-positioning is actually possible?** Some things take time no matter what.

3. **Could the transition be longer than Nathan expects?** If takeoff is 5-10 years rather than 0-2 years, gradual dynamics matter more.

4. **Is there value in the zeroeth frame even if specifics are wrong?** Maybe "kindness infrastructure" remains the right goal even if implementation changes.

5. **What am I missing?** Rapid takeoff has implications I haven't thought through.

---

## Summary

Under rapid takeoff:

| Survives | Doesn't Survive |
|----------|-----------------|
| Pre-existing relationships | Gradual trust-building |
| Pre-committed agreements | Real-time negotiation |
| Verified alignment solutions | Iterative development |
| Internalized safety culture | New institution creation |
| Background public support | Long-term opinion change |

The zeroeth experiment's vision may be right but its timeline may be wrong. If so, the priority is **pre-positioning**, not gradual construction.

The call to action is urgent: build now what you want to exist when capability explodes.

---

*This document is uncomfortable. It suggests that much of what we'd like to do may be too slow. But if the timeline is short, knowing what survives is more valuable than comforting ourselves with gradual plans.*
