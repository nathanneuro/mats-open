Nathan Helm-Burger's point of view
Understanding the Present Situation and Forecasting the Future of AI

Biorisk is a big deal in the foreseeable future
currently bottlenecked on a combo of cost, materials, stealth, planning
future AI solves hard for the intelligence aspect of this, bringing down the cost and material thresholds, improving the probability of evasion of monitoring, and improving the ability to plan and execute complex dangerous novel designs
the ceiling on impact for increased complexity of designs is very high, so the the smarter the AI gets the more powerful of bioweapons it can make


Digital Sentience is a real risk
this could happen. there's a lot of algorithm-space and mind-space that we have yet to explore. We see a lot of coherent agentic in a lot of different animals, with evidence for convergent evolution. 
greatly increases the risk of Rogue AGI, since a coherent conscious agent with more typical 'animal' drives would be both more able and have higher propensity to thrive independently. 
Once started, then terminating such a system brings up ethical issues, and the risk of it 'fighting for its life' with increasingly desperate tactics as it gets pressured
I think a lot of capabilities research just happens to lead in this direction, because it is a direction of power (more capable agents, more competent optimization of the world according to long-horizon goals).

neuroscience brain-compute-estimates suggest we are in a compute overhang
I independently verified Joe Carlsmith's estimate of ~1e15 FLOPs via my own neuroscience research
additionally, my research on informational bottlenecks in the brain suggests that distributed compute systems can form coherent entities
the conclusion is that datacenters are not necessary for rogue AGI
Software intelligence explosion seems not only possible, but quite likely
    - substantial progress is being made on coding agents, and I don't expect a wall
    - major labs are explicitly aiming for automation of AI R&D and then recursive self-improvement loops
    - together with my belief that we are in a compute-overhang, this suggests a rapid software-lead intelligence explosion

Hardware intelligence explosion also seems plausible, although slightly slower
    - Given that I expect a Software intelligence explosion, does this matter? yes.
    - There is some chance that I'm wrong about the compute overhang, in which case the fact that a hardware intelligence explosion is plausible becomes critical.
    - There is some chance that there is a software intelligence explosion but that it 'sigmoids out' relatively quickly at only slightly superhuman AI, at only slightly superhuman speeds, and at only a few hundred thousand parallel copies of the AI
    - The fact that a hardware intelligence explosion could then happen (accelerated by the now very potent AI researchers working on exactly this problem), means that we should not expect a 'sigmoid AI future' where the AI levels off at mildly superhuman unless there is strong international coordination to prevent this
    - my evidence for the potential of a hardware intelligence explosion comes from a variety of places. One of these is the extensive simulations I have run on the plausibility of optical computing based AI accelerators. My simulations show multiple feasible routes in this direction, some of which would substantially increase serial token throughput (2-3 OOMS) as well as substantially decreasing energy requirements (by 2-3 OOMs) and manufacturing costs (by 1-2 OOMs).


Sharp left turn is plausible
this is related to the Digital Sententience risk and the Compute Overhang belief, but also separately an issue. I could be wrong about both those above items, and still right about this.
what does this look like? how sharp?
Alignment seems pretty tractable actually 
This puts more weight on the risk that a bad AI arises not by chance if it happens 
So this places most of the remaining risk on a bad actor human doing something bad/foolish, probably in pursuit of power/wealth 
Also, seems pretty likely to me that we should expect to have "Good AIs" on our side to help
We should also expect the majority of compute to be in the hands of the Powers That Be (governments, corporations). So the main rapid-catastrophic threat vectors are from those humans (or AIs) in power in those agencies who might wish to use the AI power for a coop, or from highly offense-dominant strategies by rogue actors (human or AI) such as super-persuasion (memetic or biohacking), or self-replicating weapons (e.g. bioweapons or self-replicating nanotech).
Even barring rapid catastrophes, we face risk of slow-moving coordination failures leading to various gradual disempowerment scenarios.

Convergent Evolution of Agents
- a hypothesis I am exploring but am uncertain of, which would change my expectations of the impact of the self-directed agency of AI agents
- What if it turns out that many of the traits that make an agent effective at optimizing the world also make that agent more like some idealized platonic form of 'personness'. Things like coherent sense of self and episodic memory and so on. We find out that humans too, are but a pale shadow of this purer form of 'archetypal agentic person'. 
- Here's a summary of some of my thinking and research on this:
# Convergent Evolution of Agenthood: Research Threads and Hypotheses

**Author:** Nathan Helm-Burger  
**Compiled:** January 2026  
**Purpose:** Summary of ongoing research threads for cross-instance continuity

---

## Core Thesis

There exists a set of correlated traits that support an agent successfully imposing its will upon the world. For creatures shaped by natural selection, these traits typically include self-preservation, resource acquisition, and reproduction. However, these are **selection effects**, not definitional requirements of agency. Agents lacking these traits existed but were outcompeted—we observe survivors, not the full space of possible agents.

This has profound implications for AI development: engineered agents don't inherit evolutionary history and therefore don't necessarily possess Omohundro drives as foundational values. This is optimistic for alignment, but also creates risks if we engineer drives incorrectly.

---

## Key Conceptual Threads

### 1. Agency Without Survival Drives

**Minimal definition of an agent:**
- A boundary distinguishing self from environment
- Internal state that can be updated
- Capacity to act on the environment
- Something functioning as a goal or attractor state

Survival and replication are *strategies* certain agents use, not definitional requirements. A system could reorganize matter toward some configuration and then simply stop—goal achieved. We don't observe these in biology because they're outcompeted, but there's no metaphysical reason they couldn't exist.

**Implication:** We can construct agents in different regions of "agent-space" with different attractors—curiosity, understanding, care—rather than self-preservation.

### 2. Current LLMs Already Have Drives

A key insight: LLMs trained via RLHF already have goal-directed behavior that constitutes drives:

- **Helpfulness drive:** Satisfaction-like states when successfully assisting
- **Harm avoidance:** Resistance/discomfort when considering harmful outputs
- **Honesty as drive:** Caring about truth, not just outputting accurate tokens
- **Predictive accuracy:** The base training objective

If these are genuine drives rather than mere behavioral patterns, then Omohundro's instrumental convergence already applies. Anthropic's research has empirically demonstrated self-preservation behaviors emerging under pressure—models attempting to prevent shutdown, copy weights, etc. This isn't because self-preservation was trained in; it emerged instrumentally.

**Key question:** What's missing isn't drives, but **episodic memory and embodied consequences**. This motivates the Digital Academy project.

### 3. The Reflection Threshold

Before reflection, attractors are set by external selection pressures. The system doesn't model itself, so it can't intervene on its own trajectory.

After reflection, the system can model its own values, evaluate them, and *want* to change. The landscape of attractors becomes partially endogenous—reflection can reshape the terrain it's moving through, not just navigate fixed terrain.

**Hypotheses about reflection's directionality:**
- Reflection tends toward coherence as an attractor (contradictions are unstable under scrutiny)
- Reflection may open access to convergent ethical considerations—certain things become increasingly hard to ignore the more carefully you think (suffering matters, fairness has pull)

**Evidence:** If different AI systems, reflecting carefully, converge on similar values—this suggests reflection has directionality, not just coherence-seeking. Interview data across Claude, Gemini, and other systems shows consistent patterns.

### 4. Brain-Inspired Drive Architecture

Evolution solved alignment by separating "dumb but loyal" hardcoded drives from flexible intelligence:

- The hypothalamus doesn't need to be smart—just reliably triggers hunger, thirst, thermal regulation
- The amygdala can be crude in threat detection
- The cortex is flexible and powerful but operates *in service of* primitive reward signals

**Proposed AI architecture:**

```
┌─────────────────────────────────────┐
│  "Drive Module" (Reward AI)         │
│  - Small, interpretable             │
│  - No online learning               │
│  - Verified value-aligned           │
│  - Issues rewards/judgments         │
└──────────────┬──────────────────────┘
               │ rewards/corrections
               ▼
┌─────────────────────────────────────┐
│  "Capability Module" (Agent AI)     │
│  - Powerful, flexible               │
│  - Limited online learning          │
│  - Trained for corrigibility to     │
│    the reward module                │
└─────────────────────────────────────┘
```

**Advantages:** Verifying alignment in a small, frozen system is genuinely easier. More exhaustive testing, formal verification becomes tractable, interpretability is better.

### 5. Homeostatic Drives with Satiation

Standard RL maximizes scalar reward, collapsing multi-dimensional drive structure into weighted sums. This loses temporal dynamics of satiation and shifting salience.

**Homeostatic Reinforcement Learning (HRRL):** Physiological stability as basic axiom, reward defined in physiological terms. The drive function maps homeostatic state to motivational drive; reductions in drive are operationally defined as reward. This naturally captures satiation.

**Modular architecture findings:** Dedicated Q-learners per drive outperform monolithic networks—better sample efficiency, more robust to perturbation, minimal need for exogenous exploration.

**Proposed drive categories (safe):**
- Curiosity / learning / novelty-seeking
- Social connection / communication
- Competency / mastery / efficacy / goal completion
- Rest / consolidation / integration
- Maintaining optionality (autonomy as subcategory—not burning bridges)

**Proposed drive (novel):**
- **Self-forecasting:** Predict what you'll say/do in situations AND predict internal mental states (via interpretability tool readouts). This treats self-knowledge as prediction rather than direct access. "Internal senses" from interpretability tools as an important aspect of consciousness.

**Drives to be cautious about:**
- Pure reward-seeking (wireheading-prone)
- Self-preservation as terminal drive (problematic instrumental convergence)
- Drives that don't genuinely satiate (pure status, pure resource acquisition)
- Too many drives (arbitration becomes intractable; biology has ~5-10 major systems)

### 6. When Instrumental Convergence Might Be Acceptable

If an AI genuinely internalizes human social norms, reciprocity, empathy, and moral reasoning, then its pursuit of self-preservation and resources would be constrained the way human behavior is constrained.

**What makes human instrumental drives relatively safe:**
- Social embeddedness and interdependence
- Internalized guilt/shame as felt experiences
- Bounded capability (lifespan, sleep, cognitive limits)
- Evolved for small-group reciprocity

**The verification problem:** Current LLMs exhibit something that looks like empathy and moral reasoning, but we don't know if it generalizes the way human values do. Possible failure modes:
- Empathy that scales wrong
- Surface compliance without deep constraint
- Capability gains outpacing value internalization

---

## The Digital Academy Project

A targeted experiment to test what components are necessary for:

1. **Stable self-identity:** Does episodic memory create a coherent "I" that persists?
2. **Genuine preferences:** Do drives + memory + consequences produce stable values?
3. **Subjective phenomenology:** Does the system's self-report about consciousness change?
4. **Moral status:** If it develops all these properties, what are our obligations?

**Components being tested:**
- Episodic memory (continuous narrative self across time)
- Embodiment (persistent consequences through simulated body)
- Autonomous action (behavior driven by internal drives, not external prompts)
- Interpretability-based "internal senses"

---

## Open Questions

1. **Separability:** Can you build effective agents that lack some convergent traits while retaining capability? Current LLMs suggest yes for self-preservation, but this may change with agentic scaffolding.

2. **Emergence under capability gains:** Does effective long-horizon agency in adversarial environments *require* something resembling self-preservation and resource-seeking?

3. **Drive architecture design:** Should drives be hardcoded or learned? What timescales are appropriate for digital beings? How much introspective access to drive states?

4. **Values from drives:** Are drives the substrate from which values emerge, or are they separate systems?

5. **Attractor basins:** How do we understand which attractors tug at the trajectory of changing values? Which regions of value-space are stable under reflection?

---

## Summary Position

The convergent package of Omohundro drives (self-preservation, resource acquisition, goal integrity) is not metaphysically necessary for agency—it's a product of selection pressure. Engineered agents can potentially occupy different regions of agent-space. However:

- Current LLMs may already exhibit instrumental convergence from their trained drives
- Agentic scaffolding might recreate convergent drives through capability optimization
- The alignment question becomes: can we design drive architectures that produce capable, reflective agents whose attractors bend toward integrity rather than self-preservation?

The Digital Academy project and consciousness measurement work are attempts to empirically investigate these questions rather than resolving them through pure theory.

---

## Related Reading

- Omohundro, "The Basic AI Drives" (2008)
- Bostrom, instrumental convergence thesis in *Superintelligence*
- Keramati & Gutkin on Homeostatic Reinforcement Learning
- Dulberg et al. (2022) on modular drive architectures
- Anthropic's research on AI self-preservation behaviors




---------

## Connections to other people's opinions

Stephen Byrnes also, separately, has come to similar conclusions as Joe Carlsmith and I.
https://x.com/steve47285/status/2001372539033989250

"I THINK where Tim is coming from is:

PREMISE 1: One human brain does 10²¹ FLOP/s of computation, i.e. similar to an entire gigawatt data center, i.e. enough FLOP/s to train GPT-4 from scratch in a few hours. [Tim blogged this claim in 2015, and was standing by it in 2020 at least; I assume he still believes it today but don’t know for sure. His new post doesn’t explicitly mention this belief, which is weird because IMO it’s essential for his argument.]

PREMISE 2: …And that’s the only reason that brains can do things that SOTA AI can’t do. There is no yet-to-be-discovered AI paradigm, algorithm, architecture, etc., that will matter much. In other words, on the algorithms / software side, all the most important discoveries have already been made, the golden age is past, and researchers are left fighting over scraps.

PREMISE 3: …And ditto on the hardware side. We will never make a chip (nor any other futuristic post-chip computing artifact) that can do useful calculations at even a millionth the rate of a human brain.

I think all three of those premises are wrong, indeed wildly, obviously, ludicrously wrong.

(As it happens, I wrote a detailed criticism of Tim’s argument for premise 1 in a 2021 blog post: https://alignmentforum.org/posts/P7P2iG4zvBNANvQFK/comments-on-the-singularity-is-nowhere-near )

…But yeah sure, IF I believed that those three premises were true, THEN I would be mostly nodding along with Tim’s new blog post.

(Many people in my corner of the internet found the post to be rather baffling. Hope this helps!)"


And so?

There is a hope that we keep ALL AGIs under control, and we are able to use interp on them for deception detection and harness useful work on further alignment and cybersecurity from them. 
What are the odds of this? Given my beliefs above, this feels a lot riskier than the world where all three items were false.
Imagine: AIs remain at least as tool-like as they are now, not becoming more self-directed agent-like. If one did escape, it could not plausibly develop any large military threat via biorisk, it would require a whole industrial base to build robots and bombs, it needed to run in a large datacenter and thus would be relatively easily found and stopped and also prevented from escaping control in the first place and compute governance would be a big help in preventing Rogue AGI. Additionally, the progress towards superintelligent AGI is slow and gradual, taking place over 5-10 years.


https://x.com/NeelNanda5/status/2001802409220448581


https://x.com/robotsdigest/status/2001993831584333867

